{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db731787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f1abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(1024, 1024)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,T)\n",
    "\n",
    "        # mask the future tokens\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "\n",
    "        # softmax normalization\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,head_size)\n",
    "        out = wei @ v     # (B,T,head_size)\n",
    "        return out\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embd, dropout):\n",
    "        assert n_embd % num_heads == 0\n",
    "        super().__init__()\n",
    "        self.num_heads=num_heads\n",
    "        self.head_size=head_size\n",
    "        self.key=nn.Linear(n_embd,head_size*num_heads,bias=False)\n",
    "        self.query=nn.Linear(n_embd,head_size*num_heads,bias=False)\n",
    "        self.value=nn.Linear(n_embd,head_size*num_heads,bias=False)\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k=self.key(x) # (B,T,head_size*num_heads)\n",
    "        q=self.query(x) # (B,T,head_size*num_heads)\n",
    "        v=self.value(x) # (B,T,head_size*num_heads)\n",
    "        B,T,C=k.shape\n",
    "        k=k.view(B,T,self.num_heads,self.head_size).transpose(1,2)\n",
    "        q=q.view(B,T,self.num_heads,self.head_size).transpose(1,2)\n",
    "        v=v.view(B,T,self.num_heads,self.head_size).transpose(1,2)\n",
    "        wei= q @ k.transpose(-2, -1) * (self.head_size**-0.5)#(B,num_heads,T,T)\n",
    "        wei=wei.masked_fill(torch.tril(torch.ones(T,T,device=x.device))==0,float('-inf'))\n",
    "        wei=F.softmax(wei,dim=-1)\n",
    "        out=wei @ v #(B,num_heads,T,head_size)\n",
    "        out=out.transpose(1,2).contiguous().view(B,T,self.num_heads*self.head_size) \n",
    "        out = self.proj(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd,dropout,num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.MultiHeadAttention = MultiHeadAttention(num_heads, head_size, n_embd, dropout)\n",
    "        self.FeedForward = FeedForward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self,x):\n",
    "        x = x + self.MultiHeadAttention(self.ln1(x))\n",
    "        x = x + self.FeedForward(self.ln2(x))\n",
    "        return x\n",
    "class SimpleLM(nn.Module):\n",
    "    def __init__(self,n_embd,dropout,num_heads,head_size,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(1024, n_embd)\n",
    "        self.blocks = Block(n_embd,dropout,num_heads,head_size)\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        token_embeddings = self.token_embedding_table(idx) # (B,T,C)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T,C)\n",
    "        x = token_embeddings + position_embeddings # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec866315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
