{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40791bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import inspect\n",
    "import math\n",
    "master_process = (os.getenv(\"RANK\", \"0\") == \"0\")\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    vocab_size: int = 50257\n",
    "    n_embd: int = 768\n",
    "    n_head: int = 12\n",
    "    n_layer: int = 12\n",
    "    dropout: float = 0.1\n",
    "    block_size: int = 1024\n",
    "# Module\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        assert (\n",
    "            config.n_embd %config.n_head==0\n",
    "        ), \"Embedding size needs to be divisible by n_head\"\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.c_attn=nn.Linear(self.n_embd,3*self.head_dim*self.n_head)\n",
    "        self.c_proj = nn.Linear(self.n_head * self.head_dim, self.n_embd)\n",
    "        # self.scale=torch.tensor(self.head_dim ** -0.5)\n",
    "    def forward(self, x):\n",
    "        qkv=self.c_attn(x) #(B,seq_len,3*head_dim*n_head)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2) #(B,seq_len,head_dim*n_head)\n",
    "        B,seq_len,_=q.shape\n",
    "        q=q.view(B,seq_len,self.n_head,self.head_dim).transpose(1,2) #(B,n_head,seq_len,head_dim)\n",
    "        k=k.view(B,seq_len,self.n_head,self.head_dim).transpose(1,2) #(B,n_head,seq_len,head_dim)\n",
    "        # score=q @ k.transpose(-2,-1)*self.scale #(B,n_head,seq_len,seq_len)\n",
    "        # masked_mat=torch.tril(torch.ones(seq_len,seq_len,devide=x.device))\n",
    "        # masked_score=score.masked_fill(masked_mat==0,float('-inf'))\n",
    "        # weights=F.softmax(masked_score,dim=-1) #(B,n_head,seq_len,seq_len)\n",
    "        v=v.view(B,seq_len,self.n_head,self.head_dim).transpose(1,2) #(B,n_head,seq_len,head_dim)\n",
    "        # out=weights @ v #(B,n_head,seq_len,head_dim)\n",
    "        out=F.scaled_dot_product_attention(q,k,v,is_causal=True) #(B,n_head,seq_len,head_dim)\n",
    "        out=out.transpose(1,2).contiguous().view(B,seq_len,self.n_head*self.head_dim) #(B,seq_len,n_embd)\n",
    "        out=self.c_proj(out)   \n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.n_embd,4*config.n_embd)\n",
    "        self.c_proj=nn.Linear(4*config.n_embd,config.n_embd)\n",
    "        self.activation=nn.GELU(approximate=\"tanh\")\n",
    "    def forward(self,x):\n",
    "        x=self.c_fc(x)\n",
    "        x=self.activation(x)\n",
    "        x=self.c_proj(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.attn=MultiHeadAttention(config=config)\n",
    "        self.mlp=MLP(config=config)\n",
    "        self.ln_1=nn.LayerNorm(config.n_embd)\n",
    "        self.ln_2=nn.LayerNorm(config.n_embd)\n",
    "    def forward(self,x):\n",
    "        x=x+self.attn(self.ln_1(x))\n",
    "        x=x+self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "        self.transformer=nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size,config.n_embd),\n",
    "            \"wpe\": nn.Embedding(config.block_size,config.n_embd),\n",
    "            \"h\": nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            \"ln_f\": nn.LayerNorm(config.n_embd)        \n",
    "            })\n",
    "        self.lm_head=nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
    "        self.lm_head.weight=self.transformer.wte.weight\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self,idx,targets=None):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        pos=torch.arange(0,T,dtype=torch.long,device=idx.device)\n",
    "        pos_embeddings=self.transformer[\"wpe\"](pos)\n",
    "        token_embeddings=self.transformer[\"wte\"](idx)\n",
    "        x=pos_embeddings+token_embeddings\n",
    "        for block in self.transformer[\"h\"]:\n",
    "            x=block(x)\n",
    "        x=self.transformer[\"ln_f\"](x)\n",
    "        logits=self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,C=logits.shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss=F.cross_entropy(logits,targets)\n",
    "        return logits,loss\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,model_type):\n",
    "        \"\"\"Loads pretrained model weights from huggingface\"\"\"\n",
    "        assert (model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl','./gpt2'}), \"model_type must be one of 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\"\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "        config_args = {\n",
    "            './gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 \n",
    "        config_args['block_size'] = 1024 \n",
    "        config=GPT2Config(**config_args)\n",
    "        model=GPT2(config=config)\n",
    "        sd=model.state_dict()\n",
    "        sd_keys = [k for k in sd.keys() if not k.endswith('.attn.bias')]\n",
    "        model_hf=GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf=model_hf.state_dict()\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        global master_process\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bdd1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouxinkun/miniconda3/envs/deeplearning/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: ./gpt2\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "model=GPT2.from_pretrained('./gpt2')\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model=model.to(device=device)\n",
    "model=torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707fa130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1205 14:54:48.727000 2962 site-packages/torch/_inductor/utils.py:1613] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation time: 18.874134302139282 seconds\n",
      "The meaning of life is expanding, and change and change do not constitute a perpetual process like the speed of light or the amount of work required by a second religion who wants to change reality forever. That means that even the dogmatic and meta thinkers of our world cannot engage in much debate about creation – the authors' biggest content concern is the rich wealth worship in both history and economics. That reliance on \"proofing\" is amount to an epistemic defense of the objective reality for which every man concerned consists in repeating the positance clayth imperative of medieval toleration uttered only 3 hundred years before Jesus Christ. One presumes that the inspiration or the method does not warrant further exposition. At various such moments and places the survey of conceptions, therefore,\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "The meaning of life is not determined by material subjectivity; originality relies on subjectivity. Our Chapter 6: The Answer to Necessary Necessary is a study in ceteris paribus. Apart from magic, subtle skills in improving our designs or IDs — like steals and Iron pawns — are indispensable. Virtue does not extend.� Unbridled revelation and profound elemental spirits will further us down the road of acquirement, desirable to a young mind. Albert Mack Smith Frank Baer Scott Mandler NapBrew Light De Daddy Wayne Guerin American Sheen McMaster's Handbook\n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "BOOK 1 - MISTAKES IN THE BOOK OF MIZAMEL.\n",
      "\n",
      "Discover ideas, learn painful truths, become test\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "The meaning of life is important in Mormonism, that they are synthesized below in order to understand his teachings. Publication Edit\n",
      "\n",
      "Cindy Smith receives several muted citations from some of her older followers.\n",
      "\n",
      "The Great Memoir: Dick is a con man Marc said he wants to actually retire from studying supervising capitalism today. He enjoys RRT, but who needs him anyway?! [/a]\n",
      "\n",
      "\n",
      "The Vines of Living Grace - Chinese Children mostly\n",
      "\n",
      "\n",
      "Well it turns out that has probably never been written history teacher slam dunking a bunch of stuff on you and Brian art quotes straight up winnowing to complete the _good_ talearc. While floridly missing the rules, this is hardcore stupid, Earthside\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "The meaning of life is an illusion\" and an individual should strive to reflect a clear transition which will have no conflict or momentary limit.\n",
      "\n",
      "It is common sense that, playing flute, Ralf, Minica or Peter Jackson, the Roman archperors of that universe, should reflect positive, horizontal views of eastern the Great Way. Many folk known to Tinto who asked Oprah not to apply the phrase \"Alberta\" for men who had moved into whitehold, has become convinced that using the phrase \"as a casual slur\" or employing ridiculous interpretations of its usage unironically translates into godlike deferral to the main purpose, blind obedience in God's sight. Horribleness echoes most complicatedness. It is as if someone\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "The meaning of life is fitting to us.'' The quest for truth must also be conducted on an objective and principled basis, such as the imperative to police the compromises which compromise means .\n",
      "\n",
      "Every person must succeed. Every other person must fail. In the chronicles, it is often suggested that a normal person pursue a life of bliss-like luxury, as he will get closer to Boston but never to any world power, to establish his farm or his house between 2,000 and 2,500 miles away. The individual must either use his senses in the solitude or seize them, laying claim to every aspect of human existence. In the narrative, the greatest fundamental work ethics is individual virtue. It is only when positive qualities of life form the architects\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import time\n",
    "enc=tiktoken.get_encoding(\"gpt2\")\n",
    "prompt=\"The meaning of life is\"\n",
    "tokens=enc.encode(prompt)\n",
    "tokens=torch.tensor(tokens,dtype=torch.long,device=device).unsqueeze(0).repeat(5,1)\n",
    "t1=time.time()\n",
    "while tokens.shape[1]<150:\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits,_=model(tokens)\n",
    "            probs=F.softmax(logits[:,-1,:],dim=-1)\n",
    "            next_token=torch.multinomial(probs,1)\n",
    "    tokens=torch.cat([tokens,next_token],dim=-1)\n",
    "t2=time.time()\n",
    "print(f\"Generation time: {t2-t1} seconds\")\n",
    "tokens=tokens.to(\"cpu\").numpy()\n",
    "tokens=tokens.tolist()\n",
    "for i in tokens:\n",
    "    text=enc.decode(i)\n",
    "    print(text)\n",
    "    print(\"\\n------------------------------------------------------------------------------------------------------------------------\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a6f295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouxinkun/miniconda3/envs/deeplearning/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([151936, 896])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.0.input_layernorm.weight torch.Size([896])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.1.input_layernorm.weight torch.Size([896])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.2.input_layernorm.weight torch.Size([896])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.3.input_layernorm.weight torch.Size([896])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.4.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.4.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.4.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.4.input_layernorm.weight torch.Size([896])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.5.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.5.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.5.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.5.input_layernorm.weight torch.Size([896])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.6.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.6.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.6.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.6.input_layernorm.weight torch.Size([896])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.7.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.7.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.7.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.7.input_layernorm.weight torch.Size([896])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.8.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.8.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.8.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.8.input_layernorm.weight torch.Size([896])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.9.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.9.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.9.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.9.input_layernorm.weight torch.Size([896])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.10.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.10.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.10.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.10.input_layernorm.weight torch.Size([896])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.11.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.11.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.11.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.11.input_layernorm.weight torch.Size([896])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.12.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.12.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.12.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.12.input_layernorm.weight torch.Size([896])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.13.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.13.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.13.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.13.input_layernorm.weight torch.Size([896])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.14.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.14.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.14.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.14.input_layernorm.weight torch.Size([896])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.15.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.15.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.15.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.15.input_layernorm.weight torch.Size([896])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.16.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.16.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.16.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.16.input_layernorm.weight torch.Size([896])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.17.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.17.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.17.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.17.input_layernorm.weight torch.Size([896])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.18.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.18.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.18.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.18.input_layernorm.weight torch.Size([896])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.19.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.19.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.19.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.19.input_layernorm.weight torch.Size([896])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.20.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.20.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.20.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.20.input_layernorm.weight torch.Size([896])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.21.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.21.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.21.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.21.input_layernorm.weight torch.Size([896])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.22.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.22.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.22.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.22.input_layernorm.weight torch.Size([896])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([896])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([896, 896])\n",
      "model.layers.23.self_attn.q_proj.bias torch.Size([896])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([128, 896])\n",
      "model.layers.23.self_attn.k_proj.bias torch.Size([128])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([128, 896])\n",
      "model.layers.23.self_attn.v_proj.bias torch.Size([128])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([896, 896])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([4864, 896])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([4864, 896])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([896, 4864])\n",
      "model.layers.23.input_layernorm.weight torch.Size([896])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([896])\n",
      "model.norm.weight torch.Size([896])\n",
      "lm_head.weight torch.Size([151936, 896])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "model_path=\"./Qwen2.5-0.5B\"\n",
    "model_hf=AutoModelForCausalLM.from_pretrained(model_path,dtype=torch.bfloat16,device_map=\"auto\")\n",
    "sd_hf=model_hf.state_dict()\n",
    "for k,v in sd_hf.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6c041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
